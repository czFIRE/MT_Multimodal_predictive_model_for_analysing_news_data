{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer, ViTModel, BlipProcessor, BlipForQuestionAnswering , CLIPProcessor, CLIPModel, get_linear_schedule_with_warmup, AutoModelForSequenceClassification, AutoModelForImageClassification, AutoImageProcessor\n",
    "from transformers import BeitImageProcessor, BeitForImageClassification\n",
    "import pickle \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fusion approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two tensort of the size 768 and 768\n",
    "a = torch.randn(6, 768)\n",
    "b = torch.randn(6, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic element-wise multiplication\n",
    "\n",
    "d = a.mul(b)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.matmul(a, b.reshape(768, 6))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a * b) == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a + b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.einsum('ik,jk->ij', a, b)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f == (a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute einsum and normalise it\n",
    "g = torch.einsum('ij,jk->ik', a, b.T)\n",
    "g = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.einsum('ij,jk->ik', a, b.reshape(768, 6))\n",
    "i = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.ger(a[0], b[0])\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "#url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "#image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image = Image.open('./images/332177888.jpg')\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = AutoModel.from_pretrained('google/vit-base-patch16-224')\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(model.config.hidden_sizes)\n",
    "except:\n",
    "    print(\"hehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.squeeze(0)[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "### reshape tmp to 3, 224, 224\n",
    "# mp = tmp.squeeze(0)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "images = [image, image]  # Batch size 2\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/resnet-50\")\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "inputs = processor(images, return_tensors=\"pt\")\n",
    "\n",
    "# TODO: add pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = model(**inputs).last_hidden_state\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(\"apple/mobilevit-small\")\n",
    "model = AutoModel.from_pretrained(\"apple/mobilevit-small\")\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "# TODO - add pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitImageProcessor, BeitForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor shape\n",
    "BATCH_SIZE = 10\n",
    "HIDDEN_SIZE = 256\n",
    "X = 5\n",
    "Y = 5\n",
    "\n",
    "# Create a dummy input tensor with the specified shape\n",
    "input_tensor = torch.randn(BATCH_SIZE, HIDDEN_SIZE, X, Y)\n",
    "\n",
    "# Define a new layer that will transform the input tensor to the desired shape\n",
    "class TransformLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformLayer, self).__init__()\n",
    "        # Use AdaptiveAvgPool2d to perform average pooling and reduce the spatial dimensions to 1x1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        # Remove the 1x1 spatial dimensions by flattening\n",
    "        x = x.view(BATCH_SIZE, HIDDEN_SIZE)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the TransformLayer\n",
    "transform_layer = TransformLayer()\n",
    "\n",
    "# Apply the transformation to the input tensor\n",
    "output_tensor = transform_layer(input_tensor)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Output tensor shape: {output_tensor.shape}\")\n",
    "\n",
    "# Verify that the output tensor has the desired shape\n",
    "assert output_tensor.shape == (BATCH_SIZE, HIDDEN_SIZE), \"The output tensor does not have the correct shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor shape\n",
    "BATCH_SIZE = 10\n",
    "HIDDEN_SIZE = 256\n",
    "X = 5\n",
    "Y = 5\n",
    "\n",
    "# Create a dummy input tensor with the specified shape\n",
    "input_tensor = torch.randn(BATCH_SIZE, HIDDEN_SIZE, X, Y)\n",
    "\n",
    "# Define a new layer that will transform the input tensor to the desired shape\n",
    "class TransformLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, x, y):\n",
    "        super(TransformLayer, self).__init__()\n",
    "        # Calculate the number of features for the linear layer\n",
    "        self.num_features = x * y\n",
    "        # Define a linear layer that will flatten the spatial dimensions\n",
    "        self.linear = nn.Linear(self.num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the spatial dimensions of the input tensor\n",
    "        x = x.view(BATCH_SIZE, HIDDEN_SIZE, -1)\n",
    "        print(f\"Shape after reshaping: {x.shape}\")\n",
    "        # Apply the linear layer to each channel\n",
    "        x = self.linear(x)\n",
    "        print(f\"Shape after linear layer: {x.shape}\")\n",
    "        # Remove the last dimension by squeezing\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the TransformLayer\n",
    "transform_layer = TransformLayer(HIDDEN_SIZE, X, Y)\n",
    "\n",
    "# Apply the transformation to the input tensor\n",
    "output_tensor = transform_layer(input_tensor)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Output tensor shape: {output_tensor.shape}\")\n",
    "\n",
    "# Verify that the output tensor has the desired shape\n",
    "assert output_tensor.shape == (BATCH_SIZE, HIDDEN_SIZE), \"The output tensor does not have the correct shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor shape\n",
    "BATCH_SIZE = 10\n",
    "HIDDEN_SIZE = 256\n",
    "X = 5\n",
    "\n",
    "# Create a dummy input tensor with the specified shape\n",
    "input_tensor = torch.randn(BATCH_SIZE, X, HIDDEN_SIZE)\n",
    "\n",
    "# Define a new layer that will transform the input tensor to the desired shape using a pooling layer\n",
    "class PoolingTransformLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(PoolingTransformLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Use AdaptiveAvgPool1d to perform average pooling and reduce the X dimension to 1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply average pooling across the X dimension\n",
    "        x = x.transpose(1, 2)  # Swap X and HIDDEN_SIZE dimensions\n",
    "        x = self.avg_pool(x)\n",
    "        # Remove the last dimension by squeezing\n",
    "        x = x.view(BATCH_SIZE, self.hidden_size)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the TransformLayer using a pooling layer\n",
    "pooling_transform_layer = PoolingTransformLayer(HIDDEN_SIZE)\n",
    "\n",
    "# Apply the transformation to the input tensor\n",
    "output_tensor_pooling = pooling_transform_layer(input_tensor)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Output tensor shape (pooling): {output_tensor_pooling.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor shape\n",
    "BATCH_SIZE = 10\n",
    "HIDDEN_SIZE = 256\n",
    "X = 5\n",
    "\n",
    "# Create a dummy input tensor with the specified shape\n",
    "input_tensor = torch.randn(BATCH_SIZE, X, HIDDEN_SIZE)\n",
    "\n",
    "# Perform average pooling across the X dimension\n",
    "output_tensor_pooling = input_tensor.mean(dim=1)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Output tensor shape (pooling): {output_tensor_pooling.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor shape\n",
    "BATCH_SIZE = 10\n",
    "HIDDEN_SIZE = 256\n",
    "X = 5\n",
    "\n",
    "# Create a dummy input tensor with the specified shape\n",
    "input_tensor = torch.randn(BATCH_SIZE, X, HIDDEN_SIZE)\n",
    "\n",
    "# Define a new layer that will transform the input tensor to the desired shape using a linear layer\n",
    "class LinearTransformLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, x):\n",
    "        super(LinearTransformLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Define a linear layer that will flatten the spatial dimension\n",
    "        # The input features should be X * hidden_size and output features should be hidden_size\n",
    "        self.linear = nn.Linear(x * hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the spatial dimension of the input tensor\n",
    "        x = x.view(BATCH_SIZE, -1)\n",
    "        # Apply the linear layer\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the TransformLayer using a linear layer\n",
    "linear_transform_layer = LinearTransformLayer(HIDDEN_SIZE, X)\n",
    "\n",
    "# Apply the transformation to the input tensor\n",
    "output_tensor_linear = linear_transform_layer(input_tensor)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Output tensor shape (linear): {output_tensor_linear.shape}\")\n",
    "\n",
    "# Verify that the output tensor has the desired shape\n",
    "assert output_tensor_linear.shape == (BATCH_SIZE, HIDDEN_SIZE), \"The output tensor does not have the correct shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor shapes\n",
    "BATCH_SIZE = 10\n",
    "HIDDEN_SIZE = 256\n",
    "X = 5\n",
    "Y = 5\n",
    "\n",
    "# Create two dummy input tensors with the specified shapes\n",
    "input_tensor_4d = torch.randn(BATCH_SIZE, HIDDEN_SIZE, X, Y)\n",
    "input_tensor_3d = torch.randn(BATCH_SIZE, X, HIDDEN_SIZE)\n",
    "\n",
    "# Define a general function to transform the input tensor to the desired shape using a pooling layer\n",
    "def transform_tensor_pooling(input_tensor):\n",
    "    # Determine the shape of the input tensor\n",
    "    tensor_shape = input_tensor.shape\n",
    "\n",
    "    # Check if the input tensor is 4D or 3D\n",
    "    if len(tensor_shape) == 4:\n",
    "        # Use AdaptiveAvgPool2d for 4D tensor\n",
    "        avg_pool_2d = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_tensor = avg_pool_2d(input_tensor)\n",
    "        print(\"Shape after pool: \" + str(output_tensor.shape))\n",
    "        output_tensor = output_tensor.view(tensor_shape[0], tensor_shape[1])\n",
    "    elif len(tensor_shape) == 3:\n",
    "        # Use AdaptiveAvgPool1d for 3D tensor\n",
    "        avg_pool_1d = nn.AdaptiveAvgPool1d(1)\n",
    "        output_tensor = avg_pool_1d(input_tensor.transpose(1, 2))\n",
    "        print(\"Shape after pool: \" + str(output_tensor.shape))\n",
    "        output_tensor = output_tensor.view(tensor_shape[0], tensor_shape[2])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported tensor shape\")\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "# Define a general function to transform the input tensor to the desired shape using a linear layer\n",
    "def transform_tensor_linear(input_tensor):\n",
    "    # Determine the shape of the input tensor\n",
    "    tensor_shape = input_tensor.shape\n",
    "\n",
    "    # Check if the input tensor is 4D or 3D\n",
    "    if len(tensor_shape) == 4:\n",
    "        # Flatten the tensor for the linear layer\n",
    "        input_features = tensor_shape[2] * tensor_shape[3]\n",
    "        linear_layer = nn.Linear(input_features, 1)\n",
    "        output_tensor = linear_layer(input_tensor.view(tensor_shape[0], tensor_shape[1], -1))\n",
    "        output_tensor = output_tensor.squeeze(-1)\n",
    "    \n",
    "    elif len(tensor_shape) == 3:\n",
    "        # Flatten the tensor for the linear layer\n",
    "        input_features = tensor_shape[1]\n",
    "        linear_layer = nn.Linear(tensor_shape[1] * tensor_shape[2], tensor_shape[2])\n",
    "        output_tensor = linear_layer(input_tensor.view(tensor_shape[0], -1))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported tensor shape\")\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "# Example usage:\n",
    "print(transform_tensor_pooling(input_tensor_4d).shape)\n",
    "print(transform_tensor_pooling(input_tensor_3d).shape)\n",
    "\n",
    "print(transform_tensor_linear(input_tensor_4d).shape)\n",
    "print(transform_tensor_linear(input_tensor_3d).shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
