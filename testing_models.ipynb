{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the \"classification\" label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer, ViTModel, BlipProcessor, BlipForQuestionAnswering\n",
    "import pickle \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv('df_combined.csv')\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/BAAI/bge-reranker-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_limit: int = 60\n",
    "\n",
    "final_text_embeddings = []\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5').to(device)\n",
    "model.eval()\n",
    "\n",
    "filename = 'text_embeddings.obj'\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    for index in range(0, len(df_combined), size_limit):\n",
    "\n",
    "        # Sentences we want sentence embeddings for\n",
    "        # sentences = df_combined.text.to_list()[:size_limit]\n",
    "        sentences = df_combined.text.to_list()[max(0, index):min(index + size_limit, len(df_combined))]\n",
    "\n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        # for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
    "        # encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            # Perform pooling. In this case, cls pooling.\n",
    "            sentence_embeddings = model_output[\"last_hidden_state\"][:, 0]\n",
    "        # normalize embeddings\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        #print(\"Sentence embeddings:\", sentence_embeddings)\n",
    "        final_text_embeddings.append(sentence_embeddings)\n",
    "        print(index)\n",
    "\n",
    "        filehandler = open(filename, 'wb') \n",
    "        pickle.dump(final_text_embeddings, filehandler)\n",
    "        filehandler.close()\n",
    "else:\n",
    "    filehandler = open(filename, 'rb') \n",
    "    final_text_embeddings = pickle.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "final_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = sentence_embeddings[0] @ sentence_embeddings[1].T\n",
    "scores = final_text_embeddings[0][0] @ final_text_embeddings[0][1].T\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"classification_by_editorial\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"classification\"].unique().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TEMPORARY FIX, NEED TO RECOMPUTE EMBEDDINGS\n",
    "\n",
    "#non_latin_rows = df_combined[(df_combined['text'].str.contains(r'[^\\x00-\\x7F]')) | (df_combined['title'].str.contains(r'[^\\x00-\\x7F]'))]\n",
    "#df_combined = non_latin_rows\n",
    "#df_combined.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the final embeddings list into a numpy array\n",
    "final_text_embeddings = torch.cat(final_text_embeddings, dim=0)\n",
    "final_text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(384, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(256, 64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.act(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.output(x)\n",
    "        #return x\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)\n",
    "    \n",
    "nn_model = Multiclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_combined[\"classification_by_editorial\"] is a pandas Series\n",
    "labels = df_combined[\"classification_by_editorial\"]\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder and transform the labels\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Convert labels_encoded to a tensor\n",
    "labels_encoded = torch.tensor(labels_encoded).to(device)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(final_text_embeddings, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the training and test sets into PyTorch Datasets\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create DataLoaders for the training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "nn_model = Multiclass().to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):  # Number of epochs\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = nn_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        predictions.extend(predicted.tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "print('Accuracy of the model on the test set: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, predictions))\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"classification_by_editorial\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df_combined[\"classification_by_editorial\"])\n",
    "\n",
    "# Splitting the data with encoded labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_text_embeddings, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Initialize the model\n",
    "nn_model = Multiclass().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    nn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = nn_model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    nn_model.eval()\n",
    "    output = nn_model(X_test)\n",
    "    loss = criterion(output, y_test)\n",
    "    test_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]}, Test Loss: {test_losses[-1]}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    nn_model.eval()\n",
    "    output = nn_model(X_test)\n",
    "    y_test = y_test.cpu()\n",
    "    _, preds = torch.max(output, 1)\n",
    "    preds = preds.cpu()\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 of the NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define a custom classifier\n",
    "class CustomClassifier(torch.nn.Module):\n",
    "    def __init__(self, roberta_model):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.roberta = roberta_model\n",
    "        self.dense = torch.nn.Linear(768, 4)  # Roberta-base has 768 hidden units\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.roberta(**inputs).last_hidden_state\n",
    "        logits = self.dense(outputs[:, 0, :])  # Use the representation of the [CLS] token\n",
    "        return logits\n",
    "\n",
    "# Instantiate the custom classifier\n",
    "classifier = CustomClassifier(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_limit: int = 80\n",
    "filename = 'text_embeddings_roberta.obj'\n",
    "\n",
    "final_text_embeddings = []\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    for index in range(0, len(df_combined), size_limit):\n",
    "\n",
    "        # Sentences we want sentence embeddings for\n",
    "        # sentences = df_combined.text.to_list()[:size_limit]\n",
    "        sentences = df_combined.text.to_list()[max(0, index):min(index + size_limit, len(df_combined))]\n",
    "\n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input).last_hidden_state\n",
    "        # normalize embeddings\n",
    "        # sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        final_text_embeddings.append(model_output)\n",
    "        print(index)\n",
    "\n",
    "        filehandler = open(filename, 'wb') \n",
    "        pickle.dump(final_text_embeddings, filehandler)\n",
    "        filehandler.close()\n",
    "else:\n",
    "    filehandler = open(filename, 'rb') \n",
    "    final_text_embeddings = pickle.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "final_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_embeddings = torch.cat(final_text_embeddings, dim=0)\n",
    "final_text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df_combined[\"classification_by_editorial\"])\n",
    "\n",
    "embeddings = final_text_embeddings\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(embeddings, labels[:800], test_size=0.2)\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_texts = torch.stack(tuple(train_texts))\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_texts = torch.stack(tuple(test_texts))\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Train the custom model\n",
    "classifier.train()\n",
    "optimizer = torch.optim.Adam(classifier.parameters())\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    inputs = {'input_ids': train_texts}\n",
    "    logits = classifier(inputs)\n",
    "    loss = loss_fn(logits, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model's performance using the test set\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    logits = classifier(test_texts)\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    correct_predictions = (predictions == test_labels).sum().item()\n",
    "    total_predictions = test_labels.size(0)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.hidden = nn.Linear(self.roberta.config.hidden_size, 128)  # Change 128 to your desired hidden layer size\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        text_output = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output = self.drop(text_pooled_output)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output)\n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "data_subset = df_combined[:500]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(data_subset['classification_by_editorial'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(data_subset['text'], encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "MAX_LEN = 96\n",
    "\n",
    "train_data_loader = create_data_loader(train_texts, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_texts, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Initialize the classifier and optimizer\n",
    "model = Classifier(len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "# Define the number of training epochs\n",
    "EPOCHS = 2\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'STARTING Epoch {epoch + 1}/{EPOCHS}')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in test_data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_data_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss}, Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 epochs with Roberta model:\n",
    "\n",
    "```\n",
    "Accuracy: 0.9721559074299635\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.97      0.92      3908\n",
    "           1       0.99      0.98      0.98     17162\n",
    "           2       0.93      0.87      0.90      2410\n",
    "           3       0.99      0.98      0.98     17570\n",
    "\n",
    "    accuracy                           0.97     41050\n",
    "   macro avg       0.95      0.95      0.95     41050\n",
    "weighted avg       0.97      0.97      0.97     41050\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, images, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.images = images.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = Compose([Resize((224, 224)), ToTensor(), Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(\"IMAGE: \" + './images/' + str(self.images[idx]) + '.jpg')\n",
    "        image = Image.open('./images/' + str(self.images[idx]) + '.jpg').convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        #print(\"TEXTS\")\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(texts, labels, tokenizer, images, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        images=images,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.hidden = nn.Linear(self.roberta.config.hidden_size + self.vit.config.hidden_size, 128)  # Change 128 to your desired hidden layer size\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        text_output = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        image_output = self.vit(image)\n",
    "        \n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        image_pooled_output = image_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        #print(\"TEXT SHAPE: \" + str(text_pooled_output.shape))\n",
    "        #print(\"IMAGE SHAPE: \" + str(image_pooled_output.shape))\n",
    "        \n",
    "        # Simple fusion by concatenation\n",
    "        combined = torch.cat((text_pooled_output, image_pooled_output), dim=1)\n",
    "        \n",
    "        output = self.drop(combined)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output) \n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, images, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.images = images.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = Compose([Resize((224, 224)), ToTensor(), Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(\n",
    "            './images/' + str(self.images[idx]) + '.jpg').convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "\n",
    "\n",
    "def create_data_loader(texts, labels, tokenizer, images, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        images=images,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        # Change 128 to your desired hidden layer size\n",
    "        self.hidden = nn.Linear(self.roberta.config.hidden_size, 128)\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        text_output = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        image_output = self.vit(image)\n",
    "\n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        image_pooled_output = image_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Simple fusion by concatenation\n",
    "        # combined = torch.cat((text_pooled_output, image_pooled_output), dim=1)\n",
    "        # Using mul (*) operation\n",
    "        # combined = text_pooled_output.mul(image_pooled_output)\n",
    "        # using + operator\n",
    "        combined = text_pooled_output + image_pooled_output\n",
    "        # TODO: using einsum  (need to work out some bugs) \n",
    "        # combined = torch.einsum('ij,ij->ij', text_pooled_output, image_pooled_output)\n",
    "        \n",
    "        # print(\"SHAPE of combined: \" + str(combined.shape))\n",
    "\n",
    "        output = self.drop(combined)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output)\n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "data_subset = df_combined[:100]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(data_subset['classification_by_editorial'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    data_subset['text'], data_subset[\"id\"], encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "MAX_LEN = 96\n",
    "\n",
    "train_data_loader = create_data_loader(train_texts, train_labels, tokenizer, train_images, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_texts, test_labels, tokenizer, test_images, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Initialize the classifier and optimizer\n",
    "model = Classifier(len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "# Define the number of training epochs\n",
    "EPOCHS = 2\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'STARTING Epoch {epoch + 1}/{EPOCHS}')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in test_data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_data_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss}, Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    images = batch[\"image\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained multimodal models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\n",
    "    \"Salesforce/blip-vqa-capfilt-large\")\n",
    "\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(768, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 4),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "raw_image = Image.open(\n",
    "    './images/' + str(df_combined['id'][0]) + '.jpg').convert('RGB')\n",
    "\n",
    "question = \"how many dogs are in the picture?\"\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**inputs, labels=torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "image = Image.open(\n",
    "    './images/' + str(df_combined['id'][0]) + '.jpg').convert('RGB')\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "                   images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "# this is the image-text similarity score\n",
    "logits_per_image = outputs.logits_per_image\n",
    "# we can take the softmax to get the label probabilities\n",
    "probs = logits_per_image.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.text_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisualBertModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model = VisualBertModel.from_pretrained('uclanlp/visualbert-vcr')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# Sample text and image\n",
    "text = \"What color is the cat?\"\n",
    "image = torch.randn(3, 224, 224)  # This should be your actual image tensor\n",
    "\n",
    "# Prepare the inputs\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs['visual_feats'] = image.unsqueeze(0)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the last hidden state and the pooler output\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "\n",
    "# Now you can add your custom MLP classifier head on top of these outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, images):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.images = images.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(\n",
    "            './images/' + str(self.images[idx]) + '.jpg').convert('RGB')\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            'text': text,\n",
    "            'image': image,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_data_loader(texts, labels, images, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        images=images,\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "class CustomCLIPClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCLIPClassifier, self).__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.clip.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, pixel_values, labels=None):\n",
    "        outputs = self.clip(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs.pooler_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "data_subset = df_combined[:500]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(data_subset['classification_by_editorial'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    data_subset['text'], data_subset[\"id\"], encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "MAX_LEN = 96\n",
    "\n",
    "train_data_loader = create_data_loader(\n",
    "    train_texts, train_labels, train_images, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(\n",
    "    test_texts, test_labels, test_images, BATCH_SIZE)\n",
    "\n",
    "# Initialize the classifier and optimizer\n",
    "model = CustomCLIPClassifier(num_classes=4).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters\n",
    "num_epochs = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_data_loader:\n",
    "        print(batch)\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, pixel_values, labels = batch\n",
    "        loss, _ = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values, labels=labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train loss: {total_train_loss/len(train_data_loader)}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        input_ids, pixel_values, labels = batch\n",
    "        _, logits = model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fusion approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two tensort of the size 768 and 768\n",
    "a = torch.randn(6, 768)\n",
    "b = torch.randn(6, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic element-wise multiplication\n",
    "\n",
    "d = a.mul(b)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.matmul(a, b.reshape(768, 6))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a * b) == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a + b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.einsum('ik,jk->ij', a, b)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f == (a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute einsum and normalise it\n",
    "g = torch.einsum('ij,jk->ik', a, b.T)\n",
    "g = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.einsum('ij,jk->ik', a, b.reshape(768, 6))\n",
    "i = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.ger(a[0], b[0])\n",
    "h.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
