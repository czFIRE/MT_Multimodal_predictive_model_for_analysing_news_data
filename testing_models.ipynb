{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the \"classification\" label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer, ViTModel, BlipProcessor, BlipForQuestionAnswering , CLIPProcessor, CLIPModel, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\n",
    "import pickle \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>classification</th>\n",
       "      <th>time</th>\n",
       "      <th>classification_by_editorial</th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>detail</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['https://cdn.siasat.com/wp-content/uploads/20...</td>\n",
       "      <td>CAA violates Constitution, says Congress MLA U...</td>\n",
       "      <td>510044179</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.582100e+09</td>\n",
       "      <td>left_wing_outside_US</td>\n",
       "      <td>Bengaluru: Congress MLA UT Khader on Wednesday...</td>\n",
       "      <td>The Siasat Daily</td>\n",
       "      <td>Zahid Ali Khan</td>\n",
       "      <td>Zahid Ali Khan through Siasat Press</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['https://japan-forward.com/wp-content/uploads...</td>\n",
       "      <td>Toshikazu Yamanishi Vying for More Glory at th...</td>\n",
       "      <td>649460361</td>\n",
       "      <td>right_wing</td>\n",
       "      <td>1.691598e+09</td>\n",
       "      <td>right_wing_outside_US</td>\n",
       "      <td>Read the full story on SportsLook - Toshikazu ...</td>\n",
       "      <td>Japan Forward</td>\n",
       "      <td>Fuji Media Holdings</td>\n",
       "      <td>Fuji Media Holdings through Sankei Shimbun</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['https://cdn.siasat.com/wp-content/uploads/20...</td>\n",
       "      <td>Telangana: Teenmar Mallanna granted bail in ‘a...</td>\n",
       "      <td>628547521</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.681804e+09</td>\n",
       "      <td>left_wing_outside_US</td>\n",
       "      <td>Hyderabad: Teenmar Mallanna alias Naveen Chint...</td>\n",
       "      <td>The Siasat Daily</td>\n",
       "      <td>Zahid Ali Khan</td>\n",
       "      <td>Zahid Ali Khan through Siasat Press</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['https://www.newsmax.com/CMSPages/GetFile.asp...</td>\n",
       "      <td>White House Aides Interviewing to Replace FTC ...</td>\n",
       "      <td>525054416</td>\n",
       "      <td>right_wing</td>\n",
       "      <td>1.598644e+09</td>\n",
       "      <td>right_wing_in_US</td>\n",
       "      <td>The White House is currently interviewing some...</td>\n",
       "      <td>Newsmax</td>\n",
       "      <td>Christopher Ruddy</td>\n",
       "      <td>Christopher Ruddy through Newsmax Media, Inc.</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['https://apicms.thestar.com.my/uploads/images...</td>\n",
       "      <td>Bystanders rescue family after mom crashes Tes...</td>\n",
       "      <td>610701207</td>\n",
       "      <td>right_wing</td>\n",
       "      <td>1.673433e+09</td>\n",
       "      <td>right_wing_outside_US</td>\n",
       "      <td>Bystanders at a preschool jumped into a swimmi...</td>\n",
       "      <td>The Star</td>\n",
       "      <td>AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...</td>\n",
       "      <td>AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205152</th>\n",
       "      <td>['https://apicms.thestar.com.my/uploads/images...</td>\n",
       "      <td>Rugby-Australians appoint Ryles to replace Dav...</td>\n",
       "      <td>650777494</td>\n",
       "      <td>right_wing</td>\n",
       "      <td>1.692422e+09</td>\n",
       "      <td>right_wing_outside_US</td>\n",
       "      <td>FILE PHOTO: Rugby Union - Autumn Nations Cup -...</td>\n",
       "      <td>The Star</td>\n",
       "      <td>AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...</td>\n",
       "      <td>AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205153</th>\n",
       "      <td>['https://cdn.siasat.com/wp-content/uploads/20...</td>\n",
       "      <td>Saudi Arabia: Road accident kills four of Andh...</td>\n",
       "      <td>652206390</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.693123e+09</td>\n",
       "      <td>left_wing_outside_US</td>\n",
       "      <td>Jeddah: Four members of a NRI family of Andhra...</td>\n",
       "      <td>The Siasat Daily</td>\n",
       "      <td>Zahid Ali Khan</td>\n",
       "      <td>Zahid Ali Khan through Siasat Press</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205154</th>\n",
       "      <td>['https://img.haarets.co.il/bs/00000186-37f1-d...</td>\n",
       "      <td>Erdogan Fights to Save His Image No Less Than ...</td>\n",
       "      <td>619069303</td>\n",
       "      <td>left_wing</td>\n",
       "      <td>1.677369e+09</td>\n",
       "      <td>left_wing_outside_US</td>\n",
       "      <td>“What’s happening is part of fate’s plan,” Tur...</td>\n",
       "      <td>Haaretz</td>\n",
       "      <td>Schocken Family and Leonid Nevzlin</td>\n",
       "      <td>Schocken Family and Leonid Nevzlin through Haa...</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205155</th>\n",
       "      <td>['https://cdn.siasat.com/wp-content/uploads/20...</td>\n",
       "      <td>TS EAMCET BiPC 2022: 98.31% seats allotted in ...</td>\n",
       "      <td>600143320</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.667912e+09</td>\n",
       "      <td>left_wing_outside_US</td>\n",
       "      <td>Hyderabad: The Telangana State Engineering, Ag...</td>\n",
       "      <td>The Siasat Daily</td>\n",
       "      <td>Zahid Ali Khan</td>\n",
       "      <td>Zahid Ali Khan through Siasat Press</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205156</th>\n",
       "      <td>['https://www.americanprogress.org/wp-content/...</td>\n",
       "      <td>Dispatching Community Responders to 911 Calls</td>\n",
       "      <td>679772337</td>\n",
       "      <td>left_wing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_wing_in_US</td>\n",
       "      <td>Key Takeaways\\nCities use three principal mode...</td>\n",
       "      <td>Center for American Progress</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205157 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   images  \\\n",
       "0       ['https://cdn.siasat.com/wp-content/uploads/20...   \n",
       "1       ['https://japan-forward.com/wp-content/uploads...   \n",
       "2       ['https://cdn.siasat.com/wp-content/uploads/20...   \n",
       "3       ['https://www.newsmax.com/CMSPages/GetFile.asp...   \n",
       "4       ['https://apicms.thestar.com.my/uploads/images...   \n",
       "...                                                   ...   \n",
       "205152  ['https://apicms.thestar.com.my/uploads/images...   \n",
       "205153  ['https://cdn.siasat.com/wp-content/uploads/20...   \n",
       "205154  ['https://img.haarets.co.il/bs/00000186-37f1-d...   \n",
       "205155  ['https://cdn.siasat.com/wp-content/uploads/20...   \n",
       "205156  ['https://www.americanprogress.org/wp-content/...   \n",
       "\n",
       "                                                    title         id  \\\n",
       "0       CAA violates Constitution, says Congress MLA U...  510044179   \n",
       "1       Toshikazu Yamanishi Vying for More Glory at th...  649460361   \n",
       "2       Telangana: Teenmar Mallanna granted bail in ‘a...  628547521   \n",
       "3       White House Aides Interviewing to Replace FTC ...  525054416   \n",
       "4       Bystanders rescue family after mom crashes Tes...  610701207   \n",
       "...                                                   ...        ...   \n",
       "205152  Rugby-Australians appoint Ryles to replace Dav...  650777494   \n",
       "205153  Saudi Arabia: Road accident kills four of Andh...  652206390   \n",
       "205154  Erdogan Fights to Save His Image No Less Than ...  619069303   \n",
       "205155  TS EAMCET BiPC 2022: 98.31% seats allotted in ...  600143320   \n",
       "205156      Dispatching Community Responders to 911 Calls  679772337   \n",
       "\n",
       "       classification          time classification_by_editorial  \\\n",
       "0             unknown  1.582100e+09        left_wing_outside_US   \n",
       "1          right_wing  1.691598e+09       right_wing_outside_US   \n",
       "2             unknown  1.681804e+09        left_wing_outside_US   \n",
       "3          right_wing  1.598644e+09            right_wing_in_US   \n",
       "4          right_wing  1.673433e+09       right_wing_outside_US   \n",
       "...               ...           ...                         ...   \n",
       "205152     right_wing  1.692422e+09       right_wing_outside_US   \n",
       "205153        unknown  1.693123e+09        left_wing_outside_US   \n",
       "205154      left_wing  1.677369e+09        left_wing_outside_US   \n",
       "205155        unknown  1.667912e+09        left_wing_outside_US   \n",
       "205156      left_wing           NaN             left_wing_in_US   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Bengaluru: Congress MLA UT Khader on Wednesday...   \n",
       "1       Read the full story on SportsLook - Toshikazu ...   \n",
       "2       Hyderabad: Teenmar Mallanna alias Naveen Chint...   \n",
       "3       The White House is currently interviewing some...   \n",
       "4       Bystanders at a preschool jumped into a swimmi...   \n",
       "...                                                   ...   \n",
       "205152  FILE PHOTO: Rugby Union - Autumn Nations Cup -...   \n",
       "205153  Jeddah: Four members of a NRI family of Andhra...   \n",
       "205154  “What’s happening is part of fate’s plan,” Tur...   \n",
       "205155  Hyderabad: The Telangana State Engineering, Ag...   \n",
       "205156  Key Takeaways\\nCities use three principal mode...   \n",
       "\n",
       "                                name  \\\n",
       "0                   The Siasat Daily   \n",
       "1                      Japan Forward   \n",
       "2                   The Siasat Daily   \n",
       "3                            Newsmax   \n",
       "4                           The Star   \n",
       "...                              ...   \n",
       "205152                      The Star   \n",
       "205153              The Siasat Daily   \n",
       "205154                       Haaretz   \n",
       "205155              The Siasat Daily   \n",
       "205156  Center for American Progress   \n",
       "\n",
       "                                                    label  \\\n",
       "0                                          Zahid Ali Khan   \n",
       "1                                     Fuji Media Holdings   \n",
       "2                                          Zahid Ali Khan   \n",
       "3                                       Christopher Ruddy   \n",
       "4       AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...   \n",
       "...                                                   ...   \n",
       "205152  AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...   \n",
       "205153                                     Zahid Ali Khan   \n",
       "205154                 Schocken Family and Leonid Nevzlin   \n",
       "205155                                     Zahid Ali Khan   \n",
       "205156                                                NaN   \n",
       "\n",
       "                                                   detail    year  \\\n",
       "0                     Zahid Ali Khan through Siasat Press  1949.0   \n",
       "1              Fuji Media Holdings through Sankei Shimbun  2017.0   \n",
       "2                     Zahid Ali Khan through Siasat Press  1949.0   \n",
       "3           Christopher Ruddy through Newsmax Media, Inc.  1998.0   \n",
       "4       AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...  1971.0   \n",
       "...                                                   ...     ...   \n",
       "205152  AMSEC Nominees (Tempatan) Sdn. Bhd. Malaysian ...  1971.0   \n",
       "205153                Zahid Ali Khan through Siasat Press  1949.0   \n",
       "205154  Schocken Family and Leonid Nevzlin through Haa...  1918.0   \n",
       "205155                Zahid Ali Khan through Siasat Press  1949.0   \n",
       "205156                                                NaN     NaN   \n",
       "\n",
       "              country  \n",
       "0               India  \n",
       "1               Japan  \n",
       "2               India  \n",
       "3       United States  \n",
       "4            Malaysia  \n",
       "...               ...  \n",
       "205152       Malaysia  \n",
       "205153          India  \n",
       "205154         Israel  \n",
       "205155          India  \n",
       "205156            NaN  \n",
       "\n",
       "[205157 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.read_csv('df_combined.csv')\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_save_location(model_path):\n",
    "    parts = model_path.split('/', 1)  # Split at the first \"/\" encountered\n",
    "    return parts[1] if len(parts) > 1 else model_path\n",
    "\n",
    "def get_multimodal_model_save_location(nlp_model_path, image_model_path, operation):\n",
    "    nlp_parts = nlp_model_path.split('/', 1)\n",
    "    nlp_tmp = nlp_parts[1] if len(nlp_parts) > 1 else nlp_model_path\n",
    "    image_parts = image_model_path.split('/', 1)\n",
    "    image_tmp = image_parts[1] if len(image_parts) > 1 else image_model_path\n",
    "    return f\"{nlp_tmp}_{image_tmp}_{operation}\"\n",
    "\n",
    "def save_model_path(model_name):\n",
    "    return f\"./trained_models/{model_name}.pt\"\n",
    "\n",
    "def save_predictions_path(model_name):\n",
    "    return f\"./trained_results/{model_name}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/BAAI/bge-reranker-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes, pretrained_model_base):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.pretrained = pretrained_model_base\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.hidden = nn.Linear(self.pretrained.config.hidden_size, 128)  # Change 128 to your desired hidden layer size\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        text_output = self.pretrained(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output = self.drop(text_pooled_output)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output)\n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# model_path = 'roberta-base'\n",
    "# model_path = 'BAAI/bge-reranker-large'\n",
    "# model_path = 'openbmb/Eurus-RM-7b' - has problems, TODO fix\n",
    "model_path = 'facebook/bart-large-cnn'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "data_subset = df_combined#[:50]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(data_subset['classification_by_editorial'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(data_subset['text'], encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "BATCH_SIZE = 116  # maximum for BGE is ~116\n",
    "MAX_LEN = 256\n",
    "\n",
    "train_data_loader = create_data_loader(train_texts, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_texts, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Initialize the classifier and optimizer\n",
    "model = Classifier(len(le.classes_), base_model).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "# Define the number of training epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'STARTING Epoch {epoch + 1}/{EPOCHS}')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(test_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_data_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss}, Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Save model:\n",
    "torch.save(model.state_dict(), save_model_path(get_model_save_location(model_path)))\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 epochs with Roberta model:\n",
    "\n",
    "```\n",
    "Accuracy: 0.9721559074299635\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.97      0.92      3908\n",
    "           1       0.99      0.98      0.98     17162\n",
    "           2       0.93      0.87      0.90      2410\n",
    "           3       0.99      0.98      0.98     17570\n",
    "\n",
    "    accuracy                           0.97     41050\n",
    "   macro avg       0.95      0.95      0.95     41050\n",
    "weighted avg       0.97      0.97      0.97     41050\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs with BGE-Reranker large\n",
    "\n",
    "```\n",
    "Accuracy: 0.9645155000974849\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.93      0.92      3870\n",
    "           1       0.98      0.98      0.98     17201\n",
    "           2       0.82      0.86      0.84      2374\n",
    "           3       0.98      0.98      0.98     17587\n",
    "\n",
    "    accuracy                           0.96     41032\n",
    "   macro avg       0.92      0.93      0.93     41032\n",
    "weighted avg       0.97      0.96      0.96     41032\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs with BART-large-CNN\n",
    "```\n",
    "Accuracy: 0.9893985182296744\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.98      0.97      3870\n",
    "           1       0.99      0.99      0.99     17201\n",
    "           2       0.97      0.95      0.96      2374\n",
    "           3       0.99      0.99      0.99     17587\n",
    "\n",
    "    accuracy                           0.99     41032\n",
    "   macro avg       0.98      0.98      0.98     41032\n",
    "weighted avg       0.99      0.99      0.99     41032\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and evaluating trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(len(le.classes_), base_model).to(device)\n",
    "model.load_state_dict(torch.load(save_model_path(get_model_save_location(model_path))))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, images, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.images = images.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = Compose([Resize((224, 224)), ToTensor(), Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(\"IMAGE: \" + './images/' + str(self.images[idx]) + '.jpg')\n",
    "        image = Image.open('./images/' + str(self.images[idx]) + '.jpg').convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        #print(\"TEXTS\")\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(texts, labels, tokenizer, images, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        images=images,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.hidden = nn.Linear(self.roberta.config.hidden_size + self.vit.config.hidden_size, 128)  # Change 128 to your desired hidden layer size\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "        # TODO: add Xavier initialisation\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        text_output = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        image_output = self.vit(image)\n",
    "        \n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        image_pooled_output = image_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        #print(\"TEXT SHAPE: \" + str(text_pooled_output.shape))\n",
    "        #print(\"IMAGE SHAPE: \" + str(image_pooled_output.shape))\n",
    "        \n",
    "        # Simple fusion by concatenation\n",
    "        combined = torch.cat((text_pooled_output, image_pooled_output), dim=1)\n",
    "        \n",
    "        output = self.drop(combined)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output) \n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, images, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.images = images.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = Compose([Resize((224, 224)), ToTensor(), Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(\n",
    "            './images/' + str(self.images[idx]) + '.jpg').convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "\n",
    "\n",
    "def create_data_loader(texts, labels, tokenizer, images, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        images=images,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        # Change 128 to your desired hidden layer size\n",
    "        self.hidden = nn.Linear(self.roberta.config.hidden_size, 128)\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        text_output = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        image_output = self.vit(image)\n",
    "\n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        image_pooled_output = image_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Simple fusion by concatenation\n",
    "        # combined = torch.cat((text_pooled_output, image_pooled_output), dim=1)\n",
    "        # Using mul (*) operation\n",
    "        # combined = text_pooled_output.mul(image_pooled_output)\n",
    "        # using + operator\n",
    "        combined = text_pooled_output + image_pooled_output\n",
    "        # TODO: using einsum  (need to work out some bugs) \n",
    "        # combined = torch.einsum('ij,ij->ij', text_pooled_output, image_pooled_output)\n",
    "        \n",
    "        # print(\"SHAPE of combined: \" + str(combined.shape))\n",
    "\n",
    "        output = self.drop(combined)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output)\n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "data_subset = df_combined#[:100]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(data_subset['classification_by_editorial'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    data_subset['text'], data_subset[\"id\"], encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "BATCH_SIZE = 160  # 50 consumes 22GB of VRAM\n",
    "MAX_LEN = 256\n",
    "\n",
    "train_data_loader = create_data_loader(train_texts, train_labels, tokenizer, train_images, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_texts, test_labels, tokenizer, test_images, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Initialize the classifier and optimizer\n",
    "model = Classifier(len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "# Define the number of training epochs\n",
    "EPOCHS = 2\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'STARTING Epoch {epoch + 1}/{EPOCHS}')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(test_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_data_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss}, Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    images = batch[\"image\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "# Save model:\n",
    "torch.save(model.state_dict(), save_model_path(get_model_save_location(model_path)))\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 epochs of concatenation of the Roberta model and the CNN model:\n",
    "\n",
    "```\n",
    "Accuracy: 0.9848898420744785\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.98      0.96      3870\n",
    "           1       0.99      0.99      0.99     17201\n",
    "           2       0.96      0.91      0.94      2374\n",
    "           3       0.99      0.99      0.99     17587\n",
    "\n",
    "    accuracy                           0.98     41032\n",
    "   macro avg       0.97      0.97      0.97     41032\n",
    "weighted avg       0.98      0.98      0.98     41032\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained multimodal models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fusion approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two tensort of the size 768 and 768\n",
    "a = torch.randn(6, 768)\n",
    "b = torch.randn(6, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic element-wise multiplication\n",
    "\n",
    "d = a.mul(b)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.matmul(a, b.reshape(768, 6))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a * b) == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a + b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.einsum('ik,jk->ij', a, b)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f == (a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute einsum and normalise it\n",
    "g = torch.einsum('ij,jk->ik', a, b.T)\n",
    "g = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.einsum('ij,jk->ik', a, b.reshape(768, 6))\n",
    "i = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.ger(a[0], b[0])\n",
    "h.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
