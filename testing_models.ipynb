{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the \"classification\" label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer, ViTModel, BlipProcessor, BlipForQuestionAnswering , CLIPProcessor, CLIPModel, get_linear_schedule_with_warmup, AutoModelForSequenceClassification, AutoModelForImageClassification, AutoImageProcessor\n",
    "from transformers import BeitImageProcessor, BeitForImageClassification\n",
    "import pickle \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv('df_combined.csv')\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_save_location(model_path):\n",
    "    parts = model_path.split('/', 1)  # Split at the first \"/\" encountered\n",
    "    return parts[1] if len(parts) > 1 else model_path\n",
    "\n",
    "def get_multimodal_model_save_location(nlp_model_path, image_model_path, operation):\n",
    "    nlp_parts = nlp_model_path.split('/', 1)\n",
    "    nlp_tmp = nlp_parts[1] if len(nlp_parts) > 1 else nlp_model_path\n",
    "    image_parts = image_model_path.split('/', 1)\n",
    "    image_tmp = image_parts[1] if len(image_parts) > 1 else image_model_path\n",
    "    return f\"{nlp_tmp}_{image_tmp}_{operation}\"\n",
    "\n",
    "def save_model_path(model_name):\n",
    "    return f\"./trained_models/{model_name}.pt\"\n",
    "\n",
    "def save_predictions_path(model_name):\n",
    "    return f\"./trained_results/{model_name}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/BAAI/bge-reranker-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "class NLPClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, pretrained_model_base):\n",
    "        super(NLPClassifier, self).__init__()\n",
    "        self.pretrained = pretrained_model_base\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.hidden = nn.Linear(self.pretrained.config.hidden_size, 128)  # Change 128 to your desired hidden layer size\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        text_output = self.pretrained(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output = self.drop(text_pooled_output)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output)\n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_path = 'roberta-base'\n",
    "# model_path = 'BAAI/bge-reranker-large'\n",
    "# model_path = 'openbmb/Eurus-RM-7b' - has problems, TODO fix\n",
    "# model_path = 'facebook/bart-large-cnn'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "data_subset = df_combined[:50]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(data_subset['classification_by_editorial'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(data_subset['text'], encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "BATCH_SIZE = 10  # maximum for BGE is ~116\n",
    "MAX_LEN = 256\n",
    "\n",
    "train_data_loader = create_data_loader(train_texts, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_texts, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Initialize the classifier and optimizer\n",
    "model = NLPClassifier(len(le.classes_), base_model).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "# Define the number of training epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'STARTING Epoch {epoch + 1}/{EPOCHS}')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(test_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_data_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss}, Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Save model:\n",
    "torch.save(model.state_dict(), save_model_path(get_model_save_location(model_path)))\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(le.inverse_transform(true_labels), le.inverse_transform(predictions)))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 epochs with Roberta model:\n",
    "\n",
    "```\n",
    "Accuracy: 0.9721559074299635\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.97      0.92      3908\n",
    "           1       0.99      0.98      0.98     17162\n",
    "           2       0.93      0.87      0.90      2410\n",
    "           3       0.99      0.98      0.98     17570\n",
    "\n",
    "    accuracy                           0.97     41050\n",
    "   macro avg       0.95      0.95      0.95     41050\n",
    "weighted avg       0.97      0.97      0.97     41050\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs with BGE-Reranker large\n",
    "\n",
    "```\n",
    "Accuracy: 0.9645155000974849\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.93      0.92      3870\n",
    "           1       0.98      0.98      0.98     17201\n",
    "           2       0.82      0.86      0.84      2374\n",
    "           3       0.98      0.98      0.98     17587\n",
    "\n",
    "    accuracy                           0.96     41032\n",
    "   macro avg       0.92      0.93      0.93     41032\n",
    "weighted avg       0.97      0.96      0.96     41032\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs with BART-large-CNN\n",
    "```\n",
    "Accuracy: 0.9893985182296744\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.98      0.97      3870\n",
    "           1       0.99      0.99      0.99     17201\n",
    "           2       0.97      0.95      0.96      2374\n",
    "           3       0.99      0.99      0.99     17587\n",
    "\n",
    "    accuracy                           0.99     41032\n",
    "   macro avg       0.98      0.98      0.98     41032\n",
    "weighted avg       0.99      0.99      0.99     41032\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and evaluating trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NLPClassifier(len(le.classes_), base_model).to(device)\n",
    "model.load_state_dict(torch.load(save_model_path(get_model_save_location(model_path))))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(le.inverse_transform(true_labels), le.inverse_transform(predictions)))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, texts, labels, images, tokenizer, processor, max_len):\n",
    "        self.texts = texts.to_numpy()\n",
    "        self.labels = labels\n",
    "        self.images = images.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(\n",
    "            './images/' + str(self.images[idx]) + '.jpg').convert('RGB')\n",
    "        # image = self.transform(image)\n",
    "        image = self.processor(image, return_tensors='pt').pixel_values.squeeze(0)\n",
    "\n",
    "        # print(\"SHAPE of image: \" + str(image.shape))\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders\n",
    "def create_data_loader(texts, labels, tokenizer, images, processor, max_len, batch_size):\n",
    "    ds = ClassifierDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        images=images,\n",
    "        processor=processor,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "class MMClassifier(nn.Module):\n",
    "    # TODO: add hidden layer size as parameter\n",
    "    def __init__(self, n_classes, nlp_model, image_model, fusion_mode):\n",
    "        super(MMClassifier, self).__init__()\n",
    "        self.fusion_mode = fusion_mode\n",
    "\n",
    "        self.nlp_model = nlp_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "        self.hidden_layer_size = 384\n",
    "        self.hidden_size = self.get_hidden_size()\n",
    "\n",
    "        self.image_linear = nn.Linear(self.image_model.config.hidden_size, out_features=self.hidden_layer_size)\n",
    "        self.nlp_linear = nn.Linear(self.nlp_model.config.hidden_size, out_features=self.hidden_layer_size)\n",
    "        \n",
    "        self.hidden = nn.Linear(self.hidden_size, 128)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "\n",
    "    # Wrapper for the match statement for hidden size\n",
    "    def get_hidden_size(self):\n",
    "        match self.fusion_mode:\n",
    "            case 'concat':\n",
    "                return self.hidden_layer_size * 2\n",
    "            case 'mul' | 'add' | 'einsum':\n",
    "                return self.hidden_layer_size\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid fusion mode\")\n",
    "\n",
    "    def get_fused_output(self, text_pooled_output, image_pooled_output):\n",
    "        match self.fusion_mode:\n",
    "            case 'concat':\n",
    "                return torch.cat((text_pooled_output, image_pooled_output), dim=1)\n",
    "            case 'mul':\n",
    "                return text_pooled_output.mul(image_pooled_output)\n",
    "            case 'add':\n",
    "                return (text_pooled_output + image_pooled_output)/2\n",
    "            # TODO: using einsum  (need to work out some bugs) \n",
    "            case 'einsum':\n",
    "                return torch.einsum('ij,ij->ij', text_pooled_output, image_pooled_output)\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid fusion mode\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        text_output = self.nlp_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        image_output = self.image_model(image)\n",
    "\n",
    "        text_pooled_output = text_output.last_hidden_state[:, 0, :]\n",
    "        image_pooled_output = image_output.hidden_states[-1][:, 0, :]\n",
    "        \n",
    "        text_output = nn.ReLU()(self.nlp_linear(text_pooled_output))\n",
    "        image_output = nn.ReLU()(self.image_linear(image_pooled_output))\n",
    "\n",
    "        combined = self.get_fused_output(text_output, image_output)\n",
    "\n",
    "        # print(\"SHAPE of combined: \" + str(combined.shape))\n",
    "\n",
    "        output = self.drop(combined)\n",
    "        output = nn.ReLU()(self.hidden(output))\n",
    "        # return self.out(output)\n",
    "        return torch.nn.functional.log_softmax(self.out(output), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "nlp_model_path = 'roberta-base' # 'BAAI/bge-reranker-large' # 'facebook/bart-large-cnn' # 'roberta-base'\n",
    "image_model_path = 'google/vit-base-patch16-224' # 'microsoft/resnet-50' # 'google/vit-base-patch16-224'\n",
    "fusion_mode = 'mul'\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(nlp_model_path)\n",
    "nlp_model = AutoModel.from_pretrained(nlp_model_path).to(device)\n",
    "image_model = AutoModel.from_pretrained(image_model_path).to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(image_model_path)\n",
    "image_model.config.output_hidden_states = True\n",
    "\n",
    "data_subset = df_combined#[:10]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(data_subset['classification_by_editorial'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    data_subset['text'], data_subset[\"id\"], encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "BATCH_SIZE = 200  # 50 consumes 22GB of VRAM\n",
    "MAX_LEN = 256\n",
    "\n",
    "train_data_loader = create_data_loader(train_texts, train_labels, tokenizer, train_images, processor, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_texts, test_labels, tokenizer, test_images, processor, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Initialize the classifier and optimizer\n",
    "model = MMClassifier(len(le.classes_), nlp_model, image_model, fusion_mode).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "# Define the number of training epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'STARTING Epoch {epoch + 1}/{EPOCHS}')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(test_data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_data_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss}, Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Plotting the training and testing losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    images = batch[\"image\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "# Save model:\n",
    "torch.save(model.state_dict(), save_model_path(get_multimodal_model_save_location(nlp_model_path, image_model_path, fusion_mode)))\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(le.inverse_transform(true_labels), le.inverse_transform(predictions)))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs of concatenation of the Roberta model and the ViT model:\n",
    "\n",
    "```\n",
    "Accuracy: 0.986181516864886\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.95      0.98      0.96      3870\n",
    " left_wing_outside_US       0.99      0.99      0.99     17201\n",
    "     right_wing_in_US       0.97      0.93      0.95      2374\n",
    "right_wing_outside_US       0.99      0.99      0.99     17587\n",
    "\n",
    "             accuracy                           0.99     41032\n",
    "            macro avg       0.98      0.97      0.97     41032\n",
    "         weighted avg       0.99      0.99      0.99     41032\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs concat Bart-Large-CNN + ViT:\n",
    "```\n",
    "Accuracy: 0.9896419780166216\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.97      0.98      0.97      3958\n",
    " left_wing_outside_US       0.99      0.99      0.99     17133\n",
    "     right_wing_in_US       0.96      0.96      0.96      2441\n",
    "right_wing_outside_US       0.99      0.99      0.99     17499\n",
    "\n",
    "             accuracy                           0.99     41031\n",
    "            macro avg       0.98      0.98      0.98     41031\n",
    "         weighted avg       0.99      0.99      0.99     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs concat BGE-Reranker + ViT:\n",
    "```\n",
    "Accuracy: 0.9739709000511808\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.93      0.95      0.94      3958\n",
    " left_wing_outside_US       0.98      0.98      0.98     17133\n",
    "     right_wing_in_US       0.91      0.89      0.90      2441\n",
    "right_wing_outside_US       0.98      0.98      0.98     17499\n",
    "\n",
    "             accuracy                           0.97     41031\n",
    "            macro avg       0.95      0.95      0.95     41031\n",
    "         weighted avg       0.97      0.97      0.97     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs plus Bart-Large-CNN + ViT\n",
    "```\n",
    "Accuracy: 0.9897394652823475\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.98      0.98      0.98      3958\n",
    " left_wing_outside_US       0.99      0.99      0.99     17133\n",
    "     right_wing_in_US       0.96      0.96      0.96      2441\n",
    "right_wing_outside_US       0.99      0.99      0.99     17499\n",
    "\n",
    "             accuracy                           0.99     41031\n",
    "            macro avg       0.98      0.98      0.98     41031\n",
    "         weighted avg       0.99      0.99      0.99     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs mul Bart-Large-CNN + ViT\n",
    "```\n",
    "Accuracy: 0.9898125807316419\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.97      0.98      0.98      3958\n",
    " left_wing_outside_US       0.99      0.99      0.99     17133\n",
    "     right_wing_in_US       0.97      0.96      0.96      2441\n",
    "right_wing_outside_US       0.99      0.99      0.99     17499\n",
    "\n",
    "             accuracy                           0.99     41031\n",
    "            macro avg       0.98      0.98      0.98     41031\n",
    "         weighted avg       0.99      0.99      0.99     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs mul  Reranker + ViT\n",
    "```\n",
    "Accuracy: 0.952645560673637\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.90      0.88      0.89      3958\n",
    " left_wing_outside_US       0.97      0.97      0.97     17133\n",
    "     right_wing_in_US       0.79      0.80      0.80      2441\n",
    "right_wing_outside_US       0.97      0.97      0.97     17499\n",
    "\n",
    "             accuracy                           0.95     41031\n",
    "            macro avg       0.91      0.91      0.91     41031\n",
    "         weighted avg       0.95      0.95      0.95     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs add Reranker + ViT\n",
    "```\n",
    "Accuracy: 0.9728497964953328\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.93      0.94      0.94      3958\n",
    " left_wing_outside_US       0.98      0.98      0.98     17133\n",
    "     right_wing_in_US       0.90      0.88      0.89      2441\n",
    "right_wing_outside_US       0.98      0.99      0.98     17499\n",
    "\n",
    "             accuracy                           0.97     41031\n",
    "            macro avg       0.95      0.95      0.95     41031\n",
    "         weighted avg       0.97      0.97      0.97     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs add Roberta + ViT\n",
    "```\n",
    "Accuracy: 0.9869367063927275\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.96      0.98      0.97      3958\n",
    " left_wing_outside_US       0.99      0.99      0.99     17133\n",
    "     right_wing_in_US       0.96      0.94      0.95      2441\n",
    "right_wing_outside_US       0.99      0.99      0.99     17499\n",
    "\n",
    "             accuracy                           0.99     41031\n",
    "            macro avg       0.98      0.97      0.98     41031\n",
    "         weighted avg       0.99      0.99      0.99     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 epochs mul Roberta + ViT\n",
    "```\n",
    "Accuracy: 0.9834027930101631\n",
    "Classification Report:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "      left_wing_in_US       0.95      0.97      0.96      3958\n",
    " left_wing_outside_US       0.99      0.99      0.99     17133\n",
    "     right_wing_in_US       0.96      0.92      0.94      2441\n",
    "right_wing_outside_US       0.99      0.99      0.99     17499\n",
    "\n",
    "             accuracy                           0.98     41031\n",
    "            macro avg       0.97      0.97      0.97     41031\n",
    "         weighted avg       0.98      0.98      0.98     41031\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MMClassifier(len(le.classes_), nlp_model, image_model, fusion_mode).to(device)\n",
    "model.load_state_dict(torch.load(save_model_path(get_multimodal_model_save_location(nlp_model_path, image_model_path, fusion_mode))))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    images = batch[\"image\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(le.inverse_transform(true_labels), le.inverse_transform(predictions)))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained multimodal models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fusion approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two tensort of the size 768 and 768\n",
    "a = torch.randn(6, 768)\n",
    "b = torch.randn(6, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic element-wise multiplication\n",
    "\n",
    "d = a.mul(b)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.matmul(a, b.reshape(768, 6))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a * b) == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a + b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.einsum('ik,jk->ij', a, b)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f == (a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute einsum and normalise it\n",
    "g = torch.einsum('ij,jk->ik', a, b.T)\n",
    "g = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.einsum('ij,jk->ik', a, b.reshape(768, 6))\n",
    "i = torch.nn.functional.normalize(g, p=2, dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.ger(a[0], b[0])\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = AutoModel.from_pretrained('google/vit-base-patch16-224')\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.squeeze(0)[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "### reshape tmp to 3, 224, 224\n",
    "# mp = tmp.squeeze(0)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/resnet-50\")\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# TODO: add pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = model(**inputs).last_hidden_state\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(\"apple/mobilevit-small\")\n",
    "model = AutoModel.from_pretrained(\"apple/mobilevit-small\")\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "# TODO - add pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitImageProcessor, BeitForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
