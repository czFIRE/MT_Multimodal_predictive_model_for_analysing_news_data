{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Battle plan\":\n",
    "\n",
    "**Personal notes on what needs/should be done.**\n",
    "\n",
    "- Correct dtypes in both dataframes\n",
    "- Merge dataframes together\n",
    "\n",
    "- filter out blatant errors\n",
    "    how to deal with \"label pending\" / \"uncategorised\"?\n",
    "\n",
    "- first explore the ownership dataframe thoroughly - do we care that some values are missing? Since that is also a category\n",
    "\n",
    "- We definitely don't include the articles without the text\n",
    "\n",
    "- Try to find which images have a human face - use a pretrained model for that\n",
    "- Get the semantic analysis for the text\n",
    "\n",
    "- Try to predict the \"classification\" which they have based on the image input data and the rest of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600ad32",
   "metadata": {},
   "source": [
    "Relevant articles:\n",
    "\n",
    "https://ieeexplore.ieee.org/document/8715409\n",
    "\n",
    "https://arxiv.org/abs/1911.08670        https://github.com/haamoon/mmtm    MMTM model\n",
    "\n",
    "https://ieeexplore.ieee.org/abstract/document/8928538   A Survey on Canonical Correlation Analysis\n",
    "\n",
    "https://ieeexplore.ieee.org/document/9099281   Multi-Modal Stacked Denoising Autoencoder for Handling Missing Data in Healthcare Big DataÂ¨\n",
    "\n",
    "https://arxiv.org/abs/1504.06063    Multimodal Convolutional Neural Networks for Matching Image and Sentence\n",
    "\n",
    "https://arxiv.org/abs/1904.02874    An Attentive Survey of Attention Models\n",
    "\n",
    "https://arxiv.org/abs/2003.08897    Normalized and geometry-aware self-attention network for image captioning\n",
    "\n",
    "https://arxiv.org/abs/1409.0473     Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "https://link.springer.com/article/10.1007/s10489-020-01801-5   Transfer learning based hybrid 2D-3D CNN for traffic sign recognition and semantic road detection applied in advanced driver assistance systems\n",
    "\n",
    "https://arxiv.org/abs/1904.01475    Good News, Everyone! Context driven entity-aware captioning for news images   ***\n",
    "\n",
    "https://arxiv.org/abs/1903.06275    Show, Translate and Tell   ***\n",
    "\n",
    "https://arxiv.org/abs/1911.12377    Multimodal Attention Networks for Low-Level Vision-and-Language Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name\n",
    "\n",
    "The topic of the thesis could be \"multimodal learning for political classification of newsletter texts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data corresponds to the date: 08.11.2022\n",
    "\n",
    "Day before American midterm elections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7171d8-cbf3-4be2-b89f-59ca3389ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = None\n",
    "\n",
    "with open('exported_articles.json') as json_file:\n",
    "    data_original = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_keys = data_original[0].keys()\n",
    "json_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original[0]['ownership'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(data_original, key=lambda x: len(x['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf293adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original[144696]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could be done just using json.flatten but I wanted to keep the ownerships separated\n",
    "\n",
    "ownerships = [None] * len(data_original)\n",
    "missing_ownerships: list = list()\n",
    "\n",
    "for i in range(len(data_original)):\n",
    "    try:\n",
    "        ownerships[i] = data_original[i]['ownership']\n",
    "    except:\n",
    "        print('Error at index', i)\n",
    "        missing_ownerships.append(i)\n",
    "    #ownerships[i]['index'] = i\n",
    "    data_original[i] = {k: data_original[i][k] for k in json_keys - {'ownership'}}\n",
    "    #data_original[i]['index'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98245a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing ownerships:', len(missing_ownerships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed(missing_ownerships):\n",
    "    print('Removing index', i)\n",
    "    ownerships.pop(i)\n",
    "    data_original.pop(i)\n",
    "\n",
    "df_owner = pd.DataFrame(ownerships)\n",
    "df_owner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_owner.values)[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_owner.values)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_original\n",
    "\n",
    "This is the dataframe that contains information about the text and title of the articles. It also contains the \"classification\" which will serve as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.DataFrame(data_original)\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd350cbb",
   "metadata": {},
   "source": [
    "TODO: Do we care about the time variable?\n",
    "Is it really necessary for the prediction?\n",
    "\n",
    "Should we filter out the empty time? Or is it fine as is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[df_original['text'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[df_original['title'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2284fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original['images'].apply(lambda x: len(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[df_original['images'].apply(lambda x: len(x)) == 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the rows that have no text (we have to do it for both dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.drop(df_original[(df_original['text'].str.len() == 0) | (df_original['title'].str.len() == 0)].index, inplace=True)\n",
    "df_original.drop(df_original[(df_original['text'].str.len() == 0) | (df_original['title'].str.len() == 0)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct the dtypes of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the classification labels.\n",
    "\n",
    "These labels come from the overall definition of the publication source, not the article itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_original[\"classification\"].unique()))\n",
    "df_original[\"classification\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[\"classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original[\"time\"] = pd.to_datetime(df_original[\"time\"])\n",
    "df_original[\"text\"] = df_original[\"text\"].astype('string')\n",
    "df_original[\"title\"] = df_original[\"title\"].astype('string')\n",
    "df_original[\"classification\"] = df_original[\"classification\"].astype('category')\n",
    "df_original[\"classification_by_editorial\"] = df_original[\"classification_by_editorial\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[\"classification_by_editorial\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we could also do is to delete the \"image_url\" column as that won't really help us in our predictions as this basically identifies the author. If we would like to use it, we could just take the domain name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original.drop(columns=[\"image_url\"], inplace=True) # ID has to stay in since it's the image identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this should be the basic processing done for the df_original dataframe except for the \"text\" and \"title\" column. We will do that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will deal with the preprocessing of the dataframe that corresponds to the information known about the owner/creator of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip to later part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner[df_owner[\"id\"].isna()]\n",
    "\n",
    "# Question - are any of the names in the id.isna rows the same as the names in the id.notna rows?\n",
    "\n",
    "# df_owner[df_owner[\"id\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_owner[df_owner[\"id\"].isna()][\"name\"].unique()\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner[df_owner[\"name\"].isin(names) & df_owner[\"id\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part shows us that all the rows that have missing values can be reconstructed back from grouping by any column, thus that the values aren't missing at random, but they"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_owner.groupby([\"name\"], dropna=False).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_owner.groupby([\"detail\"], dropna=False).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_owner.groupby([\"address\"], dropna=False).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_owner.groupby([\"info_url\"], dropna=False).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_owners = df_owner.groupby([\"name\", \"url\", \"label\", \"detail\"], dropna=False).value_counts(dropna=False)\n",
    "grouped_owners = df_owner.groupby(\"name\", dropna=False).value_counts(dropna=False)\n",
    "grouped_owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_owners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_owner[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_owners = grouped_owners.reset_index()\n",
    "grouped_owners.rename(columns={0: \"count\"}, inplace=True)\n",
    "grouped_owners.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "grouped_owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[df_owner[\"name\"] == \"National Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_owners.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_owners[grouped_owners[\"url\"] == \"www.stanforddaily.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reconstruct the missing values in the df_owner dataframe by using the grouped_owner dataframe since all the missing values can be reconstructed from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df_owner.set_index('name').combine_first(grouped_owners.set_index('name')).reset_index()).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c691256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip to here, experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some cleaning in the data:\n",
    "\n",
    "- The column **ID** is irrelevant for machine learning, thus we can easily omit it.\n",
    "- I don't think we can get any more insight/feature from the **url** column, thus we can also drop it.\n",
    "- **Label/detail** are should be pre-processed further as more insight is needed if there is information there that could be useful.\n",
    "- **Address** could be processed to also the region of the country, but that would require a lot of effort and **country** itself should be sufficient in most of the instances.\n",
    "- **info_url** can be dropped as it doesn't contain any other relevant information (maybe we could parse the domain out of it, but I don't think that will help in any way).\n",
    "- **year** shouldn't be a variable that has a strong correlation with the predicted variable, but we will keep it for the time being to test that hypothesis.\n",
    "- **country** can be really helpful to help understand the context of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.drop(columns=[\"id\", \"url\", \"address\", \"info_url\"] ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine both df_owner and df_original together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_original, df_owner], axis=1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"label\"] = df_combined[\"label\"].astype('category')\n",
    "df_combined[\"detail\"] = df_combined[\"detail\"].astype('string')\n",
    "df_combined[\"name\"] = df_combined[\"name\"].astype('category')\n",
    "df_combined[\"country\"] = df_combined[\"country\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f513161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0e0bb",
   "metadata": {},
   "source": [
    "# Removing of articles without an image and non english articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9bc7d",
   "metadata": {},
   "source": [
    "Non english articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['text'].str.contains(r'[^\\x00-\\x7F]').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4949eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['title'].str.contains(r'[^\\x00-\\x7F]').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_latin_rows = df_combined[(df_combined['text'].str.contains(r'[^\\x00-\\x7F]')) | (df_combined['title'].str.contains(r'[^\\x00-\\x7F]'))]\n",
    "\n",
    "# df_combined = non_latin_rows\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655534fe",
   "metadata": {},
   "source": [
    "Articles with not available images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11476ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filename = \"rows_to_remove.json\"\n",
    "# Create a list to store the IDs of rows to be removed\n",
    "rows_to_remove = []\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    # List all files in the \"./images\" folder\n",
    "    image_files = os.listdir(\"./images\")\n",
    "\n",
    "    # Iterate through the IDs in the \"id\" column\n",
    "    for id_value in df_combined[\"id\"]:\n",
    "        # Check if there is no image file that begins with the ID\n",
    "        if not any(file.startswith(str(id_value)) for file in image_files):\n",
    "            rows_to_remove.append(id_value)\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(rows_to_remove, file)\n",
    "else:\n",
    "    with open(filename, \"r\") as file:\n",
    "        rows_to_remove = json.load(file)\n",
    "\n",
    "len(rows_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.drop(df_combined[df_combined['id'].isin(rows_to_remove)].index)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85207457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_owner = df_owner[(df_original['text'].str.contains(r'[^\\x00-\\x7F]')) | (df_original['title'].str.contains(r'[^\\x00-\\x7F]'))]\n",
    "#df_original = df_original[(df_original['text'].str.contains(r'[^\\x00-\\x7F]')) | (df_original['title'].str.contains(r'[^\\x00-\\x7F]'))]\n",
    "\n",
    "df_owner = df_owner.drop(df_original[df_original['id'].isin(rows_to_remove)].index)\n",
    "df_original = df_original.drop(df_original[df_original['id'].isin(rows_to_remove)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c524c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from autoviz import AutoViz_Class\n",
    "\n",
    "AV = AutoViz_Class()\n",
    "\n",
    "dft = AV.AutoViz(\n",
    "    \"\",\n",
    "    sep=\",\",\n",
    "    depVar=[\"classification_by_editorial\", \"classification\"],\n",
    "    dfte=df_combined[df_combined.columns.difference(['images', 'id', 'time'])].dropna(),\n",
    "    header=0,\n",
    "    verbose=1,\n",
    "    lowess=False,\n",
    "    chart_format=\"html\",\n",
    "    max_rows_analyzed=15000000,\n",
    "    max_cols_analyzed=30,\n",
    "    save_plot_dir=None\n",
    ")\n",
    "\"\"\"\n",
    "print(\"AutoViz is not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile = ProfileReport(df_original[df_original.columns.difference(['images', 'id', 'text'])], title=\"Profiling Report\")\n",
    "#profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile = ProfileReport(df_owner, title=\"Profiling Report\")\n",
    "#profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProfileReport(df_combined[df_combined.columns.difference(['images', 'id', 'text', 'title'])], title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add more analysis, the extra things from the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for extra features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis notes/observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Classification\" feature from the dataset\n",
    "\n",
    "The \"classification\" feature could serve as our primary \"target\".\n",
    "\n",
    "However, there are still some questions regarding this variable - mainly what does \"neutral\" mean. Because in terms of news outlets none of them can be considered neutral (unless they are strictly reporting, but even so).\n",
    "\n",
    "Are there guidelines for evaluating this variable? Can we trust that the labeling is consistent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Involved preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faces detection in images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc2622",
   "metadata": {},
   "source": [
    "## Saving the final version data after preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"df_combined.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
